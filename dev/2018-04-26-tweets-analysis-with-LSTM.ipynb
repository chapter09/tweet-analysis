{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Emotion Classification with LSTM\n",
    "Hao Wang (haowang@ece.utoronto.ca)\n",
    "April 26, 2018 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import csv\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/parsed/dev-labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_to_label = {\n",
    "    'anger': 0, \n",
    "    'fear': 1, \n",
    "    'joy': 2, \n",
    "    'love': 3, \n",
    "    'sadness': 4, \n",
    "    'surprise': 5, \n",
    "    'thankfulness': 6 \n",
    "}\n",
    "\n",
    "C = len(emo_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['emotion'].apply(lambda emo: emo_to_label[emo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "#### Cleaning:\n",
    "1. Remove @userid\n",
    "2. Remove hashtags\n",
    "3. Emoji\n",
    "4. URL\n",
    "5. Remove non-English tweets by langid library\n",
    "\n",
    "#### Tokenization & Normalization\n",
    "1. Penn Treebank tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "def preprocess_tweet(tweet):        \n",
    "#     # all caps\n",
    "#     allcaps_regex = re.compile(r\"([^a-z0-9()<>'`\\-]){2,}\")     \n",
    "#     tweet = re.sub(allcaps_regex, '\\1' + ' <allcaps> ', tweet)   \n",
    "    \n",
    "    # lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    slash_regex = re.compile(r\"/\")\n",
    "    user_regex = re.compile(r\"@[\\S]+\")\n",
    "    hash_regex = re.compile(r\"#(\\w+)\")\n",
    "    url_regex = re.compile(r\"(http|https|ftp)://[a-zA-Z0-9\\./]+\")\n",
    "    \n",
    "    emoji_heart_regex = re.compile(r\"<3\")\n",
    "    emoji_smile1_regex = re.compile(r\"[8:=;]['`\\-]?[)d]+|[)d]+['`\\-]?[8:=;]\")\n",
    "    emoji_smile2_regex = re.compile(r\"\\^(_|\\.)\\^\")\n",
    "    emoji_lol_regex = re.compile(r\"[8:=;]['`\\-]?p+\")\n",
    "    emoji_sad1_regex = re.compile(r\"[8:=;]['`\\-]?\\(+|\\)+['`\\-]?[8:=;]\")\n",
    "    emoji_sad2_regex = re.compile(r\">(_|.)<\")\n",
    "    emoji_neutral_regex = re.compile(r\"[8:=;]['`\\-]?[\\/|l*]\")\n",
    "\n",
    "    number_regex = re.compile(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\")\n",
    "    \n",
    "    # repeating punctuations \n",
    "    rpt_punc_regex = re.compile(r\"([!?.])\\1{1,}\")\n",
    "    # repeating words like hurrrryyyyyy\n",
    "    rpt_word_regex = re.compile(r\"\\b(\\S*?)(.)\\2{2,}\\b\", re.IGNORECASE)\n",
    "\n",
    "    tweet = re.sub(url_regex, ' <url> ', tweet)\n",
    "    tweet = re.sub(slash_regex, ' / ', tweet)\n",
    "    tweet = re.sub(user_regex, ' <user> ', tweet)\n",
    "    tweet = re.sub(hash_regex, ' <hashtag> ', tweet)\n",
    "    \n",
    "    tweet = re.sub(emoji_heart_regex, ' <heart> ', tweet)\n",
    "    tweet = re.sub(emoji_smile1_regex, ' <smile> ', tweet)\n",
    "    tweet = re.sub(emoji_smile2_regex, ' <smile> ', tweet)\n",
    "    tweet = re.sub(emoji_lol_regex, ' <lolface> ', tweet)\n",
    "    tweet = re.sub(emoji_sad1_regex, ' <sadface> ', tweet)\n",
    "    tweet = re.sub(emoji_sad2_regex, ' <sadface> ', tweet)\n",
    "    tweet = re.sub(emoji_neutral_regex, ' <neutralface> ', tweet)\n",
    "    \n",
    "    tweet = re.sub(number_regex, ' <number> ', tweet)\n",
    "\n",
    "    tweet = re.sub(rpt_punc_regex, r' \\1' + ' <repeat> ', tweet)\n",
    "    tweet = re.sub(rpt_word_regex, r'\\1' + r'\\2' + ' <elong> ', tweet)\n",
    "    \n",
    "    # split punctuation and words\n",
    "    word_bound_regex = re.compile(r\"(\\w+)([.,!,?]+)\")\n",
    "    tweet = re.sub(word_bound_regex, r'\\1' + r' \\2', tweet)\n",
    "    \n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    # to keep <> special word \n",
    "    tokenizer.PARENS_BRACKETS = (re.compile(r'[\\]\\[\\(\\)\\{\\}]'), r' \\g<0> ')\n",
    "    tweet_toks = tokenizer.tokenize(tweet, convert_parentheses=False)\n",
    "    \n",
    "    return tweet_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " 'a',\n",
       " 'good',\n",
       " 'night',\n",
       " 'champ',\n",
       " '(',\n",
       " ':',\n",
       " '<user>',\n",
       " 'u',\n",
       " 'were',\n",
       " 'amazing',\n",
       " 'as',\n",
       " 'always',\n",
       " '<hashtag>',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " '<number>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"damn hope i don't get out of all https://haow.ca these <3 problems coming into my future #worried\"\n",
    "s = \"Cold...Soar throat...Cofee...#feelinSick...#Sad\"\n",
    "s = \"Getting REALLY excited about @ixdconf. Passport is processing, flights are scheduled, & living is booked. What to do? #Excited #Awesomesauce\"\n",
    "s = \"Have a good night champ (: @JohnCena u were amazing as always #proud &lt;3\"\n",
    "# s = \"Done! I'm GOING home..#happiness\"\n",
    "preprocess_tweet(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the next step -- filtering non-English tweets. Our vocabulary has multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('en', -549.9064557552338)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter non-English tweets\n",
    "import langid\n",
    "EN_THRESHOLD = -420\n",
    "langid.set_languages(['en'])\n",
    "# langid.classify(\"yeah you should! And me too from high school!\")\n",
    "# langid.classify(\"dors biien en tout cas merci d'etre la pour nous t'est genial tout simplement jtai kiffÃ© dans l'episode d'aujourd'hui\")\n",
    "# langid.classify(\"I understand that they want us to feel something and be motivated to donate but do all the Bernados adverts have to be traumatising? #sad\")\n",
    "data[data['tweet'].apply(lambda t: langid.classify(t)[1] < EN_THRESHOLD)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_tk'] = data['tweet'].apply(lambda t: preprocess_tweet(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(['tweet_vt', 'tweet_tk'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing\n",
    "\n",
    "We use GloVe, a global vectors for word representation (https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "Dataset: Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = '/mnt/tweets/glove/glove.twitter.27B.200d.txt'\n",
    "dim = 200\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r',encoding='UTF-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(GLOVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['<unknown>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tk_tweet_to_indices(tweet):    \n",
    "    return [word_to_index.get(w, word_to_index['<unknown>']) for w in tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['tweet_vt'] = data['tweet_tk'].apply(lambda t: tk_tweet_to_indices(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_tk</th>\n",
       "      <th>tweet_vt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137144184007180288</td>\n",
       "      <td>i came to a realization that i am happiest i have been in a very long time. i got those two nigas i love ;) @rachelpazz &amp; myboy;) #happy.</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>[i, came, to, a, realization, that, i, am, happiest, i, have, been, in, a, very, long, time, ., i, got, those, two, nigas, i, love, &lt;smile&gt;, &lt;user&gt;, &amp;, myboy, &lt;smile&gt;, &lt;hashtag&gt;, .]</td>\n",
       "      <td>[266800, 93727, 607686, 2114, 509441, 601404, 266800, 22926, 249553, 266800, 251959, 60650, 273725, 2114, 639335, 346612, 605074, 1818, 266800, 235699, 603199, 621962, 411912, 266800, 348020, 1983, 2002, 1635, 393756, 1983, 1940, 1818]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147516714798678016</td>\n",
       "      <td>Getting REALLY excited about @ixdconf. Passport is processing, flights are scheduled, &amp; living is booked. What to do? #Excited #Awesomesauce</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>[getting, really, excited, about, &lt;user&gt;, passport, is, processing, ,, flights, are, scheduled, ,, &amp;, living, is, booked, ., what, to, do, ?, &lt;hashtag&gt;, &lt;hashtag&gt;]</td>\n",
       "      <td>[228211, 509497, 195367, 4800, 2002, 466425, 283379, 493890, 1736, 209209, 34877, 539424, 1736, 1635, 343883, 283379, 78345, 1818, 652233, 607686, 162203, 2039, 1940, 1940]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149410954986270720</td>\n",
       "      <td>1st Driving lesson in a minute #excited</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>[&lt;number&gt;, st, driving, lesson, in, a, minute, &lt;hashtag&gt;]</td>\n",
       "      <td>[1964, 572545, 167196, 339063, 273725, 2114, 380171, 1940]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142720791077863424</td>\n",
       "      <td>Have a good night champ (: @JohnCena u were amazing as always #proud &amp;lt;3</td>\n",
       "      <td>joy</td>\n",
       "      <td>2</td>\n",
       "      <td>[have, a, good, night, champ, (, :, &lt;user&gt;, u, were, amazing, as, always, &lt;hashtag&gt;, &amp;, lt, ;, &lt;number&gt;]</td>\n",
       "      <td>[251959, 2114, 234424, 412044, 103498, 1663, 1837, 2002, 623838, 651479, 23586, 37723, 22567, 1940, 1635, 348857, 1887, 1964]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135950336207761409</td>\n",
       "      <td>@topmodel_29 HOLD UP flag on the play...i feel some type of way.I've called you twice, once on your #, left 2 vm and you tweeting smh #hurt</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "      <td>[&lt;user&gt;, hold, up, flag, on, the, play, ., &lt;repeat&gt;, i, feel, some, type, of, way, .i, 've, called, you, twice, ,, once, on, your, #, ,, left, &lt;number&gt;, vm, and, you, tweeting, smh, &lt;hashtag&gt;]</td>\n",
       "      <td>[2002, 260403, 629676, 208533, 451193, 601626, 482461, 1818, 1973, 266800, 203433, 566116, 622375, 446382, 649583, 1998, 1662, 93021, 668737, 620959, 1736, 451328, 451193, 669224, 3, 1736, 337252, 1964, 643248, 26337, 668737, 620565, 562443, 1940]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tid  \\\n",
       "0  137144184007180288   \n",
       "1  147516714798678016   \n",
       "2  149410954986270720   \n",
       "3  142720791077863424   \n",
       "4  135950336207761409   \n",
       "\n",
       "                                                                                                                                          tweet  \\\n",
       "0  i came to a realization that i am happiest i have been in a very long time. i got those two nigas i love ;) @rachelpazz & myboy;) #happy.      \n",
       "1  Getting REALLY excited about @ixdconf. Passport is processing, flights are scheduled, & living is booked. What to do? #Excited #Awesomesauce   \n",
       "2  1st Driving lesson in a minute #excited                                                                                                        \n",
       "3  Have a good night champ (: @JohnCena u were amazing as always #proud &lt;3                                                                     \n",
       "4  @topmodel_29 HOLD UP flag on the play...i feel some type of way.I've called you twice, once on your #, left 2 vm and you tweeting smh #hurt    \n",
       "\n",
       "   emotion  label  \\\n",
       "0  joy      2       \n",
       "1  joy      2       \n",
       "2  joy      2       \n",
       "3  joy      2       \n",
       "4  sadness  4       \n",
       "\n",
       "                                                                                                                                                                                           tweet_tk  \\\n",
       "0  [i, came, to, a, realization, that, i, am, happiest, i, have, been, in, a, very, long, time, ., i, got, those, two, nigas, i, love, <smile>, <user>, &, myboy, <smile>, <hashtag>, .]              \n",
       "1  [getting, really, excited, about, <user>, passport, is, processing, ,, flights, are, scheduled, ,, &, living, is, booked, ., what, to, do, ?, <hashtag>, <hashtag>]                                \n",
       "2  [<number>, st, driving, lesson, in, a, minute, <hashtag>]                                                                                                                                          \n",
       "3  [have, a, good, night, champ, (, :, <user>, u, were, amazing, as, always, <hashtag>, &, lt, ;, <number>]                                                                                           \n",
       "4  [<user>, hold, up, flag, on, the, play, ., <repeat>, i, feel, some, type, of, way, .i, 've, called, you, twice, ,, once, on, your, #, ,, left, <number>, vm, and, you, tweeting, smh, <hashtag>]   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tweet_vt  \n",
       "0  [266800, 93727, 607686, 2114, 509441, 601404, 266800, 22926, 249553, 266800, 251959, 60650, 273725, 2114, 639335, 346612, 605074, 1818, 266800, 235699, 603199, 621962, 411912, 266800, 348020, 1983, 2002, 1635, 393756, 1983, 1940, 1818]              \n",
       "1  [228211, 509497, 195367, 4800, 2002, 466425, 283379, 493890, 1736, 209209, 34877, 539424, 1736, 1635, 343883, 283379, 78345, 1818, 652233, 607686, 162203, 2039, 1940, 1940]                                                                             \n",
       "2  [1964, 572545, 167196, 339063, 273725, 2114, 380171, 1940]                                                                                                                                                                                               \n",
       "3  [251959, 2114, 234424, 412044, 103498, 1663, 1837, 2002, 623838, 651479, 23586, 37723, 22567, 1940, 1635, 348857, 1887, 1964]                                                                                                                            \n",
       "4  [2002, 260403, 629676, 208533, 451193, 601626, 482461, 1818, 1973, 266800, 203433, 566116, 622375, 446382, 649583, 1998, 1662, 93021, 668737, 620959, 1736, 451328, 451193, 669224, 3, 1736, 337252, 1964, 643248, 26337, 668737, 620565, 562443, 1940]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_len = max(data['tweet_vt'].apply(lambda t: len(t)))\n",
    "\n",
    "X_indices = pad_sequences(data['tweet_vt'], max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138533, 121)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      \n",
    "        \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=True)\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetEmotion(input_shape, word_to_vec_map, word_to_index):\n",
    "    \n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(C, activation='softmax')(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(sentence_indices, X)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 121)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 121, 200)          238702800 \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 121, 128)          168448    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 121, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 239,003,735\n",
      "Trainable params: 239,003,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TweetEmotion((max_len,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C=7):\n",
    "    m = Y.shape[0]\n",
    "    oh_matrix = np.zeros((m,7))\n",
    "    \n",
    "    for i, v in data['label'].iteritems():\n",
    "        oh_matrix[i][v] = 1    \n",
    "    \n",
    "    return oh_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh = convert_to_one_hot(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_ratio = 0.8\n",
    "m = X_indices.shape[0]\n",
    "X_train_indices = X_indices[:round(m*data_train_ratio)]\n",
    "Y_train_oh = Y_oh[:round(m*data_train_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 238702800 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 16608/110826 [===>..........................] - ETA: 1:00:13 - loss: 1.8521 - acc: 0.2786"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
